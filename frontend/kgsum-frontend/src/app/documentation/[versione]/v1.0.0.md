# KGSUM v1.0.0 Documentation

## Overview

KGSUM v1.0.0 is a scalable, modular, and extensible platform for profiling and classifying Knowledge Graphs using both traditional machine learning techniques and large language models. Developed in Python with a modern React/Next.js frontend, the platform enables deep semantic analysis and intelligent categorization of RDF datasets, focusing on LOD Cloud categories. The system has been designed for research and production, with robust APIs, flexible configuration, and streamlined deployment.

---

## Architecture

### Backend

The backend is built in Python, using Flask for asynchronous API endpoints, Gunicorn for production serving, and a sophisticated data pipeline with PyTorch, Scikit-Learn, Hugging Face Transformers, spaCy, RDFLib, and pandas.

Key backend features:
- File and SPARQL endpoint ingestion.
- Automated data extraction and preprocessing.
- ML pipeline for classification and profiling.
- Support for multiple classifier types, including LLM-based approaches.
- Extensible via configuration and new scripts.

### Frontend

The frontend is a modern web app constructed with Next.js, React, Shadcn, TailwindCSS, and Typescript. It provides interactive UIs for data upload, profile visualization, documentation browsing, and user authentication (Clerk).

---

## Docker Deployment

KGSUM provides three main Docker services for full-stack deployment: backend, frontend, and GraphDB.

### Backend Dockerfile (`docker/ml/Dockerfile`)

- Starts from Miniconda base image.
- Copies all backend code, configuration, and trained models.
- Installs dependencies from `environment.yml`.
- Installs spaCy models for multi-language support (English, German, Italian, Spanish, Dutch, Portuguese, Russian, French, Chinese, Japanese, Universal Dependencies).
- Exposes port 5000 and starts with a custom boot script.

```dockerfile
FROM continuumio/miniconda3:24.11.1-0
WORKDIR /home/docker_conda_template
COPY src ./src
COPY environment.yml .
RUN conda env create --verbose -f environment.yml
RUN /opt/conda/envs/myenv/bin/python -m spacy download en_core_web_trf
EXPOSE 5000
ENTRYPOINT ["./boot.sh"]
```

### Frontend Dockerfile (`docker/web/Dockerfile`)

- Uses latest Node.js image.
- Installs npm dependencies for Next.js frontend.
- Copies and builds static assets.
- Accepts environment variables for Clerk authentication.
- Can be customized further for production deployments.

### GraphDB Dockerfile (`docker/graphdb/Dockerfile`)

- Based on Ontotext's GraphDB image.
- Copies custom `security-config.xml` for authentication and access control.
- Optionally accepts a license file.
- Exposes port 7200 for SPARQL.

```dockerfile
FROM ontotext/graphdb:11.0.1
COPY docker/graphdb/security-config.xml /opt/graphdb/dist/lib/common/WEB-INF/classes/META-INF/spring/security-config.xml
EXPOSE 7200
```

### Compose and Startup

To start all services:
```sh
cd docker
docker-compose up
```
Edit the `.env` file for all necessary environment variables (API keys, endpoints, secrets, etc).

---

## Data Extraction and Preprocessing

The data pipeline is fully scriptable and modular. Extraction scripts in `src/data_extraction` handle downloading and preparing Zenodo, GitHub, and LOD Cloud datasets. The platform supports both RDF file uploads (up to 500MB) and remote SPARQL endpoints.

### Preprocessing (`src/preprocessing.py`)

- Automated merging, cleaning, and feature extraction.
- spaCy pipelines for multi-language NER (Named Entity Recognition).
- Filtering and validation steps for dataset integrity.
- Combines data from main datasets, void descriptions, and LOV (Linked Open Vocabularies).

Example usage:
```sh
python src/preprocessing.py --gpu --no-ner --no-filter
```
Arguments:
- `--gpu` enables GPU for spaCy.
- `--no-ner` disables NER and sets `ner` field to an empty list.
- `--no-filter` disables filter checks.

Code snippet:
```python
def main(use_ner: bool = True, use_gpu: bool = False, enable_filter: bool = True) -> None:
    logger.info("Starting preprocessing workflow. NER enabled: %s, GPU enabled: %s, filter enabled: %s", use_ner, use_gpu, enable_filter)
    df = merge_dataset()
    combined_df = preprocess_combined(df, pipeline_dict, fallback_pipeline, use_ner=use_ner, enable_filter=enable_filter)
    final_df.to_json(output_path, orient="records", lines=False)
```

---

## Machine Learning Pipeline

### Training and Features

- The full pipeline can be executed with a single command:
  ```sh
  python train.py
  ```
- For stepwise control, run scripts individually (e.g., `predict_category.py`, `predict_autoencoder.py`, etc).

#### Oversampling

Oversampling is supported to handle class imbalance. It is enabled in `config.json`:
```json
{
  "training": {
    "oversample": true
  }
}
```
When enabled, the training scripts will duplicate minority class samples to ensure balanced learning.

#### TF-IDF Autoencoder

The pipeline can use TF-IDF feature vectors as input to an autoencoder neural network. This is a powerful method to extract high-level features from graph data and improve downstream classification accuracy.

Enable in `config.json`:
```json
{
  "training": {
    "use_tfidf_autoencoder": true
  }
}
```

---

## Configuration

All major workflow phases (LABELING, EXTRACTION, PROCESSING, TRAINING, STORE) and ML/classifier parameters are set in `config.json`:

```json
{
  "labeling": { "search_zenodo": true, "search_github": true, "search_lod_cloud": true },
  "extraction": { "extract_sparql": true },
  "processing": { "use_ner": true, "use_filter": true },
  "training": { "classifier": "NAIVE_BAYES", "feature": ["CURI", "PURI"], "oversample": false, "max_token": 256, "use_tfidf_autoencoder": true },
  "profile": { "store_profile_after_training": false }
}
```

Available classifiers: SVM, NAIVE_BAYES, KNN, J48, MISTRAL, MLP, DEEP, BATCHNORM

---

## API Usage

- `/api/v1/profile/file`: Profile an RDF file.
- `/api/v1/profile/sparql`: Profile from a SPARQL endpoint.
- `/apidocs`: Interactive Swagger documentation.

Example Flask endpoint:
```python
@app.route('/api/v1/profile/file', methods=['POST'])
async def profile_file():
    file = request.files.get('file')
    store = request.args.get('store', False)
    # Process RDF file with RDFLib, run ML pipeline...
    return jsonify(result), 200
```

---

## Frontend

- Run locally with:
  ```sh
  npm install
  npm run dev
  ```
- React/Next.js web app with Markdown documentation viewer, authentication (optional), and interactive UIs.
- Documentation and changelogs rendered from Markdown files in `frontend/kgsum-frontend/src/app/documentation`.

---

## Hardware Requirements

- **Minimum**: 32GB RAM, RTX 3070 GPU.
- **Recommended**: 64GB RAM, RTX 3090 or NVIDIA AI GPU (16GB+ VRAM).
- LLM training and autoencoder development require CUDA and significant VRAM.

---

## Extensibility

KGSUM is designed for extensibility. New classifiers and preprocessing steps can be added by editing `config.json` and relevant Python scripts. spaCy pipelines can be further customized for additional languages or tasks.  
To document changes or new versions, add a Markdown file in `frontend/kgsum-frontend/src/app/documentation` following the version convention (e.g., `v1.1.0.md`).

---

## Contact & Acknowledgments

Supervisors: Prof. Maria Angela Pellegrino, Gabriele Tuozzo  
Author: Mario Cosenza  
Email: cosenzamario@proton.me

Special thanks to the University of Salerno ISISLab

---

For questions, issues, or contributions, refer to the [GitHub repository](https://github.com/mariocosenza/kgsum) or open an issue.

---